{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeec2b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7d4db9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_df(oaqm, cond_raw, pred):\n",
    "    \"\"\"\n",
    "    Feature engineers training dataset from sensor data. Merges weather data, creates cyclical time \n",
    "    features, removes feature not being trained on.\n",
    "    oaqm: oaqm sensor data\n",
    "    cond_raw: weathe target variable\n",
    "    Returns: training dataset\n",
    "    \"\"\"\n",
    "    # currenlty handling missing PM2.5 data by just removing it  \n",
    "    data = oaqm.dropna(axis=0, subset=[pred])\n",
    "    \n",
    "    data = data[['temperature','pressure','humidity','dB','mslPressure', pred]]\n",
    "    \n",
    "    cond = cond_raw[['rain','wdsp','wddir','sun','vis','clht','clamt']]\n",
    "    \n",
    "    # Creating the cyclical features\n",
    "    data['hour_sin'] = np.sin(2 * np.pi * data.index.hour/24)\n",
    "    data['hour_cos'] = np.cos(2 * np.pi * data.index.hour/24)\n",
    "    data['day_of_week_sin'] = np.sin(2 * np.pi * data.index.dayofweek/7)\n",
    "    data['day_of_week_cos'] = np.cos(2 * np.pi * data.index.dayofweek/7)\n",
    "    \n",
    "    x = pd.merge(data, cond, how='inner', right_index=True, left_index=True)\n",
    "    \n",
    "    # interpolate missing data\n",
    "    columns_with_nans = x.columns[x.isnull().any()].tolist()\n",
    "    for column in columns_with_nans:\n",
    "#         print(f'Filling NA in: {column}')\n",
    "        x[column] = x[column].interpolate(method='time')\n",
    "        # After interpolation, if NaNs remain, forward fill or backward fill\n",
    "        x[column].fillna(method='ffill', inplace=True)\n",
    "        x[column].fillna(method='bfill', inplace=True)\n",
    "\n",
    "    y = x[pred]\n",
    "    x.drop(columns=pred, inplace=True)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a598f48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_train(X_train, y_train, n_estimators=100, max_features=4):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Training the model\n",
    "    rf = RandomForestRegressor(n_estimators=n_estimators, max_features=max_features, random_state=42, oob_score=True)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e7c8b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_parameter_tune(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Tunes \n",
    "    \"\"\"\n",
    "    n_estimators_options = [100, 200, 300, 500, 1000]\n",
    "    max_features_options = range(2,8)\n",
    "    #max depthtrees??\n",
    "\n",
    "    # Store the performance results\n",
    "    performance_results = {}\n",
    "\n",
    "    # Loop over the parameters\n",
    "    for n_estimators in n_estimators_options:\n",
    "        for max_features in max_features_options:\n",
    "            rf = rf_train(X_train, y_train, n_estimators=n_estimators, max_features=max_features)\n",
    "            # Store the OOB score\n",
    "            performance_results[(n_estimators, max_features)] = rf.oob_score_\n",
    "\n",
    "    # Find the best parameters based on OOB score\n",
    "    best_params = max(performance_results, key=performance_results.get)\n",
    "    best_oob_score = performance_results[best_params]\n",
    "\n",
    "    print(\"Best Parameters (n_estimators, max_features):\", best_params)\n",
    "    print(\"Best OOB Score:\", best_oob_score)\n",
    "    \n",
    "    return best_params[0], best_params[1], best_oob_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a04d6d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_train_all(oaqms, pred):\n",
    "    \"\"\"\n",
    "    Trains, hyperparamter tunes, and predicts list of sensors\n",
    "    Returns: dictionary of sensors with nested dictionary of attributes: train_split_test, parameters, \n",
    "    obb_score, feature_importances, predictions,\n",
    "    \"\"\"\n",
    "    scrf = dict()\n",
    "    \n",
    "    for sensor in oaqms:\n",
    "        rfattr = dict()\n",
    "        \n",
    "        data = oaqms[sensor]\n",
    "        X, y = train_df(data, weather, pred)\n",
    "        \n",
    "        # Splitting the dataset\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        print(f\"{sensor}\\n\")\n",
    "        # hyperparameter tuning\n",
    "        n_est, max_feature, obb = rf_parameter_tune(X_train, y_train)\n",
    "        # fitting the model\n",
    "        rf = rf_train(X_train, y_train, n_est, max_feature)\n",
    "        \n",
    "        # Making predictions on the test set\n",
    "        predictions = rf.predict(X_test)\n",
    "\n",
    "        # storing into dict\n",
    "        rfattr['X_train'] = X_train\n",
    "        rfattr['y_train'] = y_train\n",
    "        rfattr['X_test'] = X_test\n",
    "        rfattr['y_test'] = y_test\n",
    "        rfattr['n_estimator'] = n_est\n",
    "        rfattr['max_feature'] = max_feature\n",
    "        rfattr['obb_score'] = obb\n",
    "        rfattr['rf'] = rf\n",
    "        rfattr['feature_scores'] = rf.feature_importances_\n",
    "        rfattr['y_preds'] = predictions\n",
    "\n",
    "        scrf[sensor] = rfattr\n",
    "        \n",
    "    return scrf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89fe8ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rf(scrf):\n",
    "    \"\"\"\n",
    "    Evaluates the test prediction scores for all sensors in the scrf dictionary.\n",
    "    scrf: A dictionary with sensors as keys and dictionaries containing 'y_preds' and 'y_test' as values.\n",
    "    Returns: A dictionary with sensors as keys and their evaluation scores as values.\n",
    "    \"\"\"\n",
    "    evaluation_results = {}\n",
    "    \n",
    "    for sensor, data in scrf.items():\n",
    "        # Retrieve predictions and actual values\n",
    "        y_preds = data['y_preds']\n",
    "        y_test = data['y_test']\n",
    "        \n",
    "        # Calculate evaluation metrics\n",
    "        mae = mean_absolute_error(y_test, y_preds)\n",
    "        mse = mean_squared_error(y_test, y_preds)\n",
    "        rmse = mean_squared_error(y_test, y_preds, squared=False)  # Set squared=False for RMSE\n",
    "        r2 = r2_score(y_test, y_preds)\n",
    "        \n",
    "        # Store the results\n",
    "        evaluation_results[sensor] = {\n",
    "            'MAE': mae,\n",
    "            'MSE': mse,\n",
    "            'RMSE': rmse,\n",
    "            'R2': r2\n",
    "        }\n",
    "        \n",
    "    return evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aeac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(sensorsrf):\n",
    "    \"\"\"\n",
    "    sensorsrf: dictionary of random forest results of every sensor\n",
    "    \"\"\"\n",
    "    # Setup the subplot grid\n",
    "    num_rows = 2\n",
    "    num_cols = 3\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 10))  # Adjust the figure size as needed\n",
    "    axes = axes.flatten()  # Flatten the array of axes for easy iterating\n",
    "\n",
    "    # Iterate over each sensor and create a subplot for its feature importances\n",
    "    for i, (sensor, data) in enumerate(sensorsrf.items()):\n",
    "        # Extract feature importances and sort them\n",
    "        importances = pd.Series(data['feature_scores'], index=data['X_train'].columns)\n",
    "        importances_sorted = importances.sort_values()\n",
    "\n",
    "        # Plot bar chart on the appropriate subplot\n",
    "        axes[i].barh(importances_sorted.index, importances_sorted, color='skyblue')\n",
    "        axes[i].set_title(f'{sensor}')\n",
    "        axes[i].set_xlabel('Relative Importance')\n",
    "        axes[i].set_ylabel('Features')\n",
    "\n",
    "    # Hide any unused subplots if you have less than 6 sensors\n",
    "    for j in range(i + 1, num_rows * num_cols):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    # Adjust layout for better readability\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d837a9ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
